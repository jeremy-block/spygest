{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import libraries\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "PID = 1\n",
    "dataset = 1\n",
    "numBreakpoints = 11\n",
    "startText = ['Arms', 'Terrorist', 'Disappearance']\n",
    "filename = '../data/Dataset_' + str(dataset) + '/User Words/p' + str(PID) + '.csv'\n",
    "breakpointsFile = '../data/Dataset_' + str(dataset) + '/Segmentation/' + startText[dataset - 1] + '_P' + str(PID) +'_'+str(numBreakpoints)+ '_Prov_Segments.csv'\n",
    "\n",
    "print(\">>> getting words in \"+filename)\n",
    "with open(filename, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    f.close()\n",
    "    print(len(data[0]),\"words in vocabulary\")\n",
    "    print(len(data))\n",
    "    \n",
    "print(\">>> getting the breakpoint values from segmentation:\",breakpointsFile)\n",
    "with open(breakpointsFile, newline='') as f2:\n",
    "    breakpoints = np.genfromtxt(f2, delimiter=',',dtype=int,usecols=4,skip_header=1)\n",
    "    f2.close()\n",
    "    print(breakpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f9b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aggregate histograms\n",
    "print(np.shape(data))\n",
    "#for each event\n",
    "for i in range(len(data)):\n",
    "    #Skip the first column (since this is the time of the event)\n",
    "    if i != 0:    \n",
    "        #for each word referenced in that event\n",
    "        for j in range(len(data[i])):\n",
    "            #make sure it is cast as a float value\n",
    "            data[i][j] = float(data[i][j])\n",
    "\n",
    "# print(len(data))\n",
    "aggregateHist = []\n",
    "currentDoc = 1\n",
    "for segment in breakpoints:\n",
    "    # print(\"for segment: \"+str(segment))\n",
    "    currentHist = data[currentDoc]\n",
    "    numDocs = 1\n",
    "    for docPos in range(currentDoc + 1, segment):\n",
    "        for word in range(len(data[docPos])):\n",
    "            currentHist[word] = float(currentHist[word]) + float(data[docPos][word])\n",
    "            numDocs += 1\n",
    "    # for wordPos in range(len(currentHist)):\n",
    "    #     currentHist[wordPos] = float(currentHist[wordPos]) / float(numDocs)\n",
    "    aggregateHist.append(currentHist)\n",
    "    currentDoc = segment\n",
    "print(len(aggregateHist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67268e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify top words\n",
    "topFivesIndices = []\n",
    "#for each segment\n",
    "# print(aggregateHist[0])\n",
    "for i in range(len(aggregateHist)):\n",
    "    indexList = []\n",
    "    #get a set of the 25 most unique words.\n",
    "    for j in range(0, 25):\n",
    "        #set the time of the event to 0 so it doesn't get flagged as the max\n",
    "        aggregateHist[i][0] = 0 \n",
    "        # Identify the word with the most frequency\n",
    "        max_index = aggregateHist[i].index(max(aggregateHist[i]))\n",
    "        #add the index of that word to a list\n",
    "        indexList.append(max_index)\n",
    "        #set the value to zero so we don't pick it again.\n",
    "        aggregateHist[i][max_index] = 0\n",
    "    #add the new set of words to the top five list.\n",
    "    topFivesIndices.append(indexList)\n",
    "\n",
    "topFives = []\n",
    "for i in range(len(topFivesIndices)):\n",
    "    wordList = []\n",
    "    for j in range(len(topFivesIndices[i])):\n",
    "        wordList.append(data[0][topFivesIndices[i][j]])\n",
    "    topFives.append(wordList)\n",
    "print(np.shape(topFives))\n",
    "print(topFives[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove non-words and stems\n",
    "# Remove words with non-alpha characters\n",
    "for i in range(len(topFives)):\n",
    "    for j in range(len(topFives[i])):\n",
    "        if topFives[i][j].isalpha() == False:\n",
    "            topFives[i][j] = \"\"\n",
    "        if len(topFives[i][j]) <= 2:\n",
    "            topFives[i][j] = \"\"\n",
    "        # if any(map(str.isdigit, topFives[i][j])) == True:\n",
    "        #     # print(topFives[i][j])\n",
    "        #     topFives[i][j] = \"\"\n",
    "        #     # print(topFives[i][j])\n",
    "\n",
    "# remove words with the same stem (e.g., textbook and textbooks)\n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"intTime\")\n",
    "ps = PorterStemmer()\n",
    "for i in range(len(topFives)):\n",
    "    for j in range(len(topFives[i])):\n",
    "        if topFives[i][j] in stop_words:\n",
    "            topFives[i][j] = \"\"\n",
    "        for k in range(len(topFives[i])):\n",
    "            if topFives[i][j] != topFives[i][k] and ps.stem(topFives[i][j]) == ps.stem(topFives[i][k]):\n",
    "                topFives[i][k] = \"\"\n",
    "\n",
    "for i in range(len(topFives)):\n",
    "    while \"\" in topFives[i]:\n",
    "        topFives[i].remove(\"\")\n",
    "\n",
    "topFivesOnly = []\n",
    "for i in range(len(topFives)):\n",
    "    topFivesOnly.append(topFives[i][0:5])\n",
    "print(topFivesOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outFilename = '../data/Dataset_' + str(dataset) + '/segKeys/' + startText[dataset - 1] + '_P' + str(PID) +'_'+str(len(breakpoints))+ '_keys.csv'\n",
    "os.makedirs(os.path.dirname(outFilename), exist_ok=True)\n",
    "with open(outFilename, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(topFivesOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124a973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
